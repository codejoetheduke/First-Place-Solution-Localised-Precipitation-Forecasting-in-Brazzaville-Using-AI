{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "339cf19f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T20:02:18.648851Z",
     "iopub.status.busy": "2025-08-03T20:02:18.648474Z",
     "iopub.status.idle": "2025-08-03T20:02:18.658923Z",
     "shell.execute_reply": "2025-08-03T20:02:18.657929Z"
    },
    "papermill": {
     "duration": 0.017201,
     "end_time": "2025-08-03T20:02:18.660514",
     "exception": false,
     "start_time": "2025-08-03T20:02:18.643313",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dataloader.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dataloader.py\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def load_data(train_path, test_path):\n",
    "    \"\"\"\n",
    "    Loads training and testing datasets as Pandas DataFrames.\n",
    "    \n",
    "    Parameters:\n",
    "    - train_path (str): Path to the training CSV file.\n",
    "    - test_path (str): Path to the testing CSV file.\n",
    "    \n",
    "    Returns:\n",
    "    - train_df (pd.DataFrame): Training dataset.\n",
    "    - test_df (pd.DataFrame): Testing dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        train_df = pd.read_csv(train_path)\n",
    "        test_df = pd.read_csv(test_path)\n",
    "        print(f\"‚úÖ Data loaded successfully!\\n - Train shape: {train_df.shape}\\n - Test shape: {test_df.shape}\")\n",
    "        return train_df, test_df\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading data: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4db87791",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T20:02:18.668999Z",
     "iopub.status.busy": "2025-08-03T20:02:18.668723Z",
     "iopub.status.idle": "2025-08-03T20:02:18.676455Z",
     "shell.execute_reply": "2025-08-03T20:02:18.675000Z"
    },
    "papermill": {
     "duration": 0.01371,
     "end_time": "2025-08-03T20:02:18.678291",
     "exception": false,
     "start_time": "2025-08-03T20:02:18.664581",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocessing.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "import statistics\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class TimeSeriesFeatureEngineer:\n",
    "    def __init__(self, \n",
    "                 date_col='DATE', \n",
    "                 target_col='Target',\n",
    "                 diff_cols=None,\n",
    "                 n_lags=2):\n",
    "        \"\"\"\n",
    "        A class for creating lag and difference features in a combined train/test dataset.\n",
    "\n",
    "        Parameters:\n",
    "        - date_col: str, name of the date column\n",
    "        - target_col: str, name of the target column\n",
    "        - diff_cols: list, columns to compute first-order differences for\n",
    "        - n_lags: int, number of lag/lead features to create\n",
    "        \"\"\"\n",
    "        self.date_col = date_col\n",
    "        self.target_col = target_col\n",
    "        self.diff_cols = diff_cols if diff_cols else []\n",
    "        self.n_lags = n_lags\n",
    "\n",
    "    def transform(self, train_df, test_df):\n",
    "        # Sort and tag\n",
    "        train_df = train_df.sort_values(self.date_col).reset_index(drop=True)\n",
    "        test_df = test_df.sort_values(self.date_col).reset_index(drop=True)\n",
    "        train_df['Set'] = 'train'\n",
    "        test_df['Set'] = 'test'\n",
    "        \n",
    "        # Combine datasets\n",
    "        dataset = pd.concat([train_df, test_df], axis=0).sort_values(self.date_col).reset_index(drop=True)\n",
    "\n",
    "        # Lag and lead features\n",
    "        for lag in range(1, self.n_lags + 1):\n",
    "            dataset[f'{self.target_col}_Lag_{lag}'] = dataset[self.target_col].shift(lag)\n",
    "            dataset[f'{self.target_col}_Lead_{lag}'] = dataset[self.target_col].shift(-lag)\n",
    "\n",
    "        # First-order difference features\n",
    "        for col in self.diff_cols:\n",
    "            dataset[f'{col}_diff1'] = dataset[col].diff()\n",
    "\n",
    "        # Split back into train and test\n",
    "        train_processed = dataset[dataset['Set'] == 'train'].copy()\n",
    "        test_processed = dataset[dataset['Set'] == 'test'].copy()\n",
    "\n",
    "        # Drop helper column\n",
    "        train_processed.drop(columns=['Set'], inplace=True)\n",
    "        test_processed.drop(columns=['Set'], inplace=True)\n",
    "        \n",
    "        return train_processed, test_processed\n",
    "\n",
    "\n",
    "def prepare_features(train_df, test_df, target_col='Target', drop_cols=None):\n",
    "    \"\"\"\n",
    "    Prepares X, y, and X_test by dropping specified columns.\n",
    "    \n",
    "    Parameters:\n",
    "    - train_df: pd.DataFrame, training dataset\n",
    "    - test_df: pd.DataFrame, test dataset\n",
    "    - target_col: str, name of target column\n",
    "    - drop_cols: list, columns to drop (in addition to target)\n",
    "    \n",
    "    Returns:\n",
    "    - X: pd.DataFrame, training features\n",
    "    - y: pd.Series, training target\n",
    "    - X_test: pd.DataFrame, test features\n",
    "    \"\"\"\n",
    "    if drop_cols is None:\n",
    "        drop_cols = []\n",
    "    \n",
    "    # Ensure target is in drop columns\n",
    "    cols_to_drop = list(set(drop_cols + [target_col]))\n",
    "\n",
    "    # Extract target\n",
    "    y = train_df[target_col]\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    X = train_df.drop(columns=cols_to_drop, errors='ignore')\n",
    "    X_test = test_df[X.columns]  # Ensure same columns as X\n",
    "\n",
    "    return X, y, X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99ee6185",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T20:02:18.686264Z",
     "iopub.status.busy": "2025-08-03T20:02:18.685928Z",
     "iopub.status.idle": "2025-08-03T20:02:18.696611Z",
     "shell.execute_reply": "2025-08-03T20:02:18.695557Z"
    },
    "papermill": {
     "duration": 0.016813,
     "end_time": "2025-08-03T20:02:18.698417",
     "exception": false,
     "start_time": "2025-08-03T20:02:18.681604",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing models.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile models.py\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "import statistics\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class CatBoostCV:\n",
    "    def __init__(self, \n",
    "                 n_splits=10, \n",
    "                 random_state=42, \n",
    "                 use_groups=False,\n",
    "                 model_params=None):\n",
    "        \"\"\"\n",
    "        CatBoost cross-validation trainer with tqdm progress.\n",
    "\n",
    "        Parameters:\n",
    "        - n_splits: int, number of folds\n",
    "        - random_state: int, random seed\n",
    "        - use_groups: bool, whether to use GroupKFold\n",
    "        - model_params: dict, CatBoost hyperparameters\n",
    "        \"\"\"\n",
    "        self.n_splits = n_splits\n",
    "        self.random_state = random_state\n",
    "        self.use_groups = use_groups\n",
    "        self.model_params = model_params if model_params else {\n",
    "            'iterations': 1000,\n",
    "            'loss_function': 'RMSE',\n",
    "            'eval_metric': 'RMSE',\n",
    "            'early_stopping_rounds': 100,\n",
    "            'verbose': 0,\n",
    "            'random_seed': random_state\n",
    "        }\n",
    "        self.models = []\n",
    "        self.oof_preds = None\n",
    "        self.test_preds = None\n",
    "        self.rmse_list = []\n",
    "\n",
    "    def fit(self, X, y, X_test, groups=None):\n",
    "        \"\"\"\n",
    "        Fits the model using cross-validation and stores OOF/test predictions.\n",
    "\n",
    "        Parameters:\n",
    "        - X: pd.DataFrame, training features\n",
    "        - y: pd.Series, target\n",
    "        - X_test: pd.DataFrame, test features\n",
    "        - groups: pd.Series or array, group labels for GroupKFold (if used)\n",
    "        \"\"\"\n",
    "        # Choose splitter\n",
    "        if self.use_groups:\n",
    "            if groups is None:\n",
    "                raise ValueError(\"Groups must be provided for GroupKFold.\")\n",
    "            splitter = GroupKFold(n_splits=self.n_splits)\n",
    "            split_data = splitter.split(X, y, groups)\n",
    "        else:\n",
    "            splitter = KFold(n_splits=self.n_splits, shuffle=True, random_state=self.random_state)\n",
    "            split_data = splitter.split(X, y)\n",
    "\n",
    "        # Initialize arrays\n",
    "        self.oof_preds = np.zeros(len(X))\n",
    "        self.test_preds = np.zeros(len(X_test))\n",
    "\n",
    "        # Progress bar\n",
    "        for train_idx, val_idx in tqdm(split_data, total=self.n_splits, desc=\"Training Folds\"):\n",
    "            X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "            train_pool = Pool(X_tr, y_tr)\n",
    "            val_pool = Pool(X_val, y_val)\n",
    "\n",
    "            # Model\n",
    "            model = CatBoostRegressor(**self.model_params)\n",
    "            model.fit(train_pool, eval_set=val_pool, use_best_model=True)\n",
    "            self.models.append(model)\n",
    "\n",
    "            # OOF predictions\n",
    "            self.oof_preds[val_idx] = model.predict(X_val)\n",
    "            rmse = mean_squared_error(y_val, self.oof_preds[val_idx], squared=False)\n",
    "            self.rmse_list.append(rmse)\n",
    "\n",
    "            # Test predictions\n",
    "            test_pool = Pool(X_test)\n",
    "            self.test_preds += model.predict(test_pool) / self.n_splits\n",
    "\n",
    "        print(f\"\\nMean CV RMSE: {np.mean(self.rmse_list):.4f}\")\n",
    "        return self\n",
    "\n",
    "    def get_oof(self):\n",
    "        return self.oof_preds\n",
    "\n",
    "    def get_test_preds(self):\n",
    "        return self.test_preds\n",
    "\n",
    "    def get_models(self):\n",
    "        return self.models\n",
    "\n",
    "class LightGBMCV:\n",
    "    def __init__(self, \n",
    "                 n_splits=10, \n",
    "                 random_state=42, \n",
    "                 use_groups=False,\n",
    "                 model_params=None,\n",
    "                 num_boost_round=1000):\n",
    "        \"\"\"\n",
    "        LightGBM cross-validation trainer with tqdm progress.\n",
    "        \n",
    "        Parameters:\n",
    "        - n_splits: int, number of folds\n",
    "        - random_state: int, random seed\n",
    "        - use_groups: bool, whether to use GroupKFold\n",
    "        - model_params: dict, LightGBM hyperparameters\n",
    "        - num_boost_round: int, max boosting rounds\n",
    "        \"\"\"\n",
    "        self.n_splits = n_splits\n",
    "        self.random_state = random_state\n",
    "        self.use_groups = use_groups\n",
    "        self.num_boost_round = num_boost_round\n",
    "        self.model_params = model_params if model_params else {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'rmse',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'learning_rate': 0.05,\n",
    "            'num_leaves': 64,\n",
    "            'max_depth': -1,\n",
    "            'feature_fraction': 0.9,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'bagging_freq': 5,\n",
    "            'verbose': -1,\n",
    "            'random_state': random_state,\n",
    "            'early_stopping_rounds': 100,\n",
    "        }\n",
    "        self.models = []\n",
    "        self.oof_preds = None\n",
    "        self.test_preds = None\n",
    "        self.rmse_list = []\n",
    "\n",
    "    def fit(self, X, y, X_test, groups=None):\n",
    "        \"\"\"\n",
    "        Fits the model using cross-validation and stores OOF/test predictions.\n",
    "        \"\"\"\n",
    "        # Choose splitter\n",
    "        if self.use_groups:\n",
    "            if groups is None:\n",
    "                raise ValueError(\"Groups must be provided for GroupKFold.\")\n",
    "            splitter = GroupKFold(n_splits=self.n_splits)\n",
    "            split_data = splitter.split(X, y, groups)\n",
    "        else:\n",
    "            splitter = KFold(n_splits=self.n_splits, shuffle=True, random_state=self.random_state)\n",
    "            split_data = splitter.split(X, y)\n",
    "\n",
    "        # Initialize arrays\n",
    "        self.oof_preds = np.zeros(len(X))\n",
    "        self.test_preds = np.zeros(len(X_test))\n",
    "\n",
    "        # Progress bar\n",
    "        for train_idx, val_idx in tqdm(split_data, total=self.n_splits, desc=\"Training Folds\"):\n",
    "            X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "            train_set = lgb.Dataset(X_tr, label=y_tr)\n",
    "            val_set = lgb.Dataset(X_val, label=y_val, reference=train_set)\n",
    "\n",
    "            model = lgb.train(\n",
    "                self.model_params,\n",
    "                train_set,\n",
    "                num_boost_round=self.num_boost_round,\n",
    "                valid_sets=[train_set, val_set],\n",
    "                valid_names=['train', 'valid'],\n",
    "            )\n",
    "            self.models.append(model)\n",
    "\n",
    "            # OOF predictions\n",
    "            self.oof_preds[val_idx] = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "            rmse = mean_squared_error(y_val, self.oof_preds[val_idx], squared=False)\n",
    "            self.rmse_list.append(rmse)\n",
    "\n",
    "            # Test predictions\n",
    "            self.test_preds += model.predict(X_test, num_iteration=model.best_iteration) / self.n_splits\n",
    "\n",
    "        print(f\"\\nMean CV RMSE: {np.mean(self.rmse_list):.4f} ¬± {np.std(self.rmse_list):.4f}\")\n",
    "        return self\n",
    "\n",
    "    def get_oof(self):\n",
    "        return self.oof_preds\n",
    "\n",
    "    def get_test_preds(self):\n",
    "        return self.test_preds\n",
    "\n",
    "    def get_models(self):\n",
    "        return self.models\n",
    "        \n",
    "\n",
    "class StackingRegressor:\n",
    "    def __init__(self, meta_model=None, n_splits=10, random_state=42):\n",
    "        \"\"\"\n",
    "        Stacks multiple model predictions using a meta-model.\n",
    "\n",
    "        Parameters:\n",
    "        - meta_model: sklearn-style regressor (default: Ridge regression)\n",
    "        - n_splits: int, number of CV folds for stacking\n",
    "        - random_state: int, reproducibility\n",
    "        \"\"\"\n",
    "        self.meta_model = meta_model if meta_model else Ridge(alpha=1.0)\n",
    "        self.n_splits = n_splits\n",
    "        self.random_state = random_state\n",
    "        self.oof_preds = None\n",
    "        self.test_preds = None\n",
    "        self.rmse_list = []\n",
    "        self.models = []\n",
    "\n",
    "    def fit(self, oof_list, y, test_list):\n",
    "        \"\"\"\n",
    "        Fits the stacking model.\n",
    "\n",
    "        Parameters:\n",
    "        - oof_list: list of arrays (OOF predictions from base models)\n",
    "        - y: Series or array, true target values\n",
    "        - test_list: list of arrays (test predictions from base models)\n",
    "        \"\"\"\n",
    "        # Combine OOF & test predictions\n",
    "        X_stack = np.column_stack(oof_list)\n",
    "        X_test_stack = np.column_stack(test_list)\n",
    "\n",
    "        # KFold for meta-model training\n",
    "        kf = KFold(n_splits=self.n_splits, shuffle=True, random_state=self.random_state)\n",
    "        self.oof_preds = np.zeros(len(y))\n",
    "        self.test_preds = np.zeros(len(X_test_stack))\n",
    "\n",
    "        for train_idx, val_idx in tqdm(kf.split(X_stack, y), total=self.n_splits, desc=\"Stacking Folds\"):\n",
    "            X_tr, X_val = X_stack[train_idx], X_stack[val_idx]\n",
    "            y_tr, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "            model = self._clone_model()\n",
    "            model.fit(X_tr, y_tr)\n",
    "            self.models.append(model)\n",
    "\n",
    "            # OOF predictions\n",
    "            self.oof_preds[val_idx] = model.predict(X_val)\n",
    "            rmse = mean_squared_error(y_val, self.oof_preds[val_idx], squared=False)\n",
    "            self.rmse_list.append(rmse)\n",
    "\n",
    "            # Test predictions\n",
    "            self.test_preds += model.predict(X_test_stack) / self.n_splits\n",
    "\n",
    "        print(f\"\\nStacking CV RMSE: {np.mean(self.rmse_list):.4f} ¬± {np.std(self.rmse_list):.4f}\")\n",
    "        return self\n",
    "\n",
    "    def _clone_model(self):\n",
    "        \"\"\"Re-initialize the meta-model for each fold.\"\"\"\n",
    "        return type(self.meta_model)(**self.meta_model.get_params())\n",
    "\n",
    "    def get_oof(self):\n",
    "        return self.oof_preds\n",
    "\n",
    "    def get_test_preds(self):\n",
    "        return self.test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dd9a8d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T20:02:18.706307Z",
     "iopub.status.busy": "2025-08-03T20:02:18.705972Z",
     "iopub.status.idle": "2025-08-03T20:02:18.712834Z",
     "shell.execute_reply": "2025-08-03T20:02:18.711777Z"
    },
    "papermill": {
     "duration": 0.01254,
     "end_time": "2025-08-03T20:02:18.714341",
     "exception": false,
     "start_time": "2025-08-03T20:02:18.701801",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils.py\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "import statistics\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def ensemble_predictions(pred_list, weights=None):\n",
    "    \"\"\"\n",
    "    Averages multiple prediction arrays into one ensemble prediction.\n",
    "    \n",
    "    Parameters:\n",
    "    - pred_list: list of np.arrays, each containing model predictions\n",
    "    - weights: list of floats, weights for each model (must sum to 1). If None, equal weights are used.\n",
    "    \n",
    "    Returns:\n",
    "    - np.array: ensembled predictions\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        weights = [1 / len(pred_list)] * len(pred_list)\n",
    "    else:\n",
    "        if not np.isclose(sum(weights), 1.0):\n",
    "            raise ValueError(\"Weights must sum to 1.\")\n",
    "    \n",
    "    pred_list = [np.array(p) for p in pred_list]\n",
    "    ensemble = np.zeros_like(pred_list[0], dtype=float)\n",
    "    for p, w in zip(pred_list, weights):\n",
    "        ensemble += w * p\n",
    "    return ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20375b83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T20:02:18.722431Z",
     "iopub.status.busy": "2025-08-03T20:02:18.722141Z",
     "iopub.status.idle": "2025-08-03T20:02:18.727998Z",
     "shell.execute_reply": "2025-08-03T20:02:18.727130Z"
    },
    "papermill": {
     "duration": 0.011574,
     "end_time": "2025-08-03T20:02:18.729427",
     "exception": false,
     "start_time": "2025-08-03T20:02:18.717853",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.py\n",
    "\n",
    "TRAIN_PATH = \"/kaggle/input/localised-precipitation-forecasting/Train_data.csv\"\n",
    "TEST_PATH = \"/kaggle/input/localised-precipitation-forecasting/Test_data.csv\"\n",
    "\n",
    "DIFF_COLUMNS = ['RH2M', 'QV2M', 'T2MDEW', 'PS']\n",
    "DROP_COLUMNS_CAT = ['Set','DATE', 'ID', 'DY']\n",
    "DROP_COLUMNS_LGB = ['Set','DATE', 'ID']\n",
    "\n",
    "N_SPLITS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5f9b77c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T20:02:18.738063Z",
     "iopub.status.busy": "2025-08-03T20:02:18.737744Z",
     "iopub.status.idle": "2025-08-03T20:02:18.745128Z",
     "shell.execute_reply": "2025-08-03T20:02:18.744082Z"
    },
    "papermill": {
     "duration": 0.013537,
     "end_time": "2025-08-03T20:02:18.746821",
     "exception": false,
     "start_time": "2025-08-03T20:02:18.733284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataloader import load_data\n",
    "from preprocessing import TimeSeriesFeatureEngineer, prepare_features\n",
    "from models import CatBoostCV, LightGBMCV, StackingRegressor\n",
    "from utils import ensemble_predictions\n",
    "from config import *\n",
    "from sklearn.linear_model import Ridge\n",
    "import os\n",
    "\n",
    "def run_training():\n",
    "    # Ensure artifacts directory exists\n",
    "    os.makedirs(\"artifacts\", exist_ok=True)\n",
    "    # Load data\n",
    "    train_df, test_df = load_data(TRAIN_PATH, TEST_PATH)\n",
    "\n",
    "    # Feature engineering\n",
    "    fe = TimeSeriesFeatureEngineer(date_col='DATE', target_col='Target', diff_cols=DIFF_COLUMNS, n_lags=2)\n",
    "    train_processed, test_processed = fe.transform(train_df, test_df)\n",
    "    joblib.dump(fe, \"artifacts/fe.pkl\")\n",
    "    print(\"artifacts/fe.pkl\")\n",
    "\n",
    "    # CatBoost\n",
    "    X_cb, y_cb, X_test_cb = prepare_features(train_processed, test_processed, target_col='Target', drop_cols=DROP_COLUMNS_CAT)\n",
    "    cb_plain = CatBoostCV(n_splits=N_SPLITS, use_groups=False)\n",
    "    cb_plain.fit(X_cb, y_cb, X_test_cb)\n",
    "    cb_group = CatBoostCV(n_splits=N_SPLITS, use_groups=True)\n",
    "    cb_group.fit(X_cb, y_cb, X_test_cb, groups=train_df.get('YEAR'))\n",
    "    cb_preds = ensemble_predictions([cb_plain.get_test_preds(), cb_group.get_test_preds()])\n",
    "    joblib.dump((cb_plain, cb_group), \"artifacts/cb_model.pkl\")\n",
    "\n",
    "    # LightGBM\n",
    "    X_lgb, y_lgb, X_test_lgb = prepare_features(train_processed, test_processed, target_col='Target', drop_cols=DROP_COLUMNS_LGB)\n",
    "    lgb_plain = LightGBMCV(n_splits=N_SPLITS, use_groups=False)\n",
    "    lgb_plain.fit(X_lgb, y_lgb, X_test_lgb)\n",
    "    lgb_group = LightGBMCV(n_splits=N_SPLITS, use_groups=True)\n",
    "    lgb_group.fit(X_lgb, y_lgb, X_test_lgb, groups=train_df.get('YEAR'))\n",
    "    lgb_preds = ensemble_predictions([lgb_plain.get_test_preds(), lgb_group.get_test_preds()])\n",
    "    joblib.dump((lgb_plain, lgb_group), \"artifacts/lgb_model.pkl\")\n",
    "\n",
    "    # Stacking\n",
    "    oof_list = [(cb_plain.get_oof() + cb_group.get_oof()) / 2, (lgb_plain.get_oof() + lgb_group.get_oof()) / 2]\n",
    "    test_list = [cb_preds, lgb_preds]\n",
    "    stacker = StackingRegressor(meta_model=Ridge(alpha=1), n_splits=N_SPLITS)\n",
    "    stacker.fit(oof_list, y_lgb.values, test_list)\n",
    "    joblib.dump(stacker, \"artifacts/stacker.pkl\")\n",
    "\n",
    "    print(\"‚úÖ Training complete. Models saved in 'artifacts/'.\")\n",
    "\n",
    "def run_inference(output_path=\"predictions.csv\"):\n",
    "    # Load data\n",
    "    train_df, test_df = load_data(TRAIN_PATH, TEST_PATH)\n",
    "\n",
    "    # Load preprocessing and models\n",
    "    fe = joblib.load(\"artifacts/fe.pkl\")\n",
    "    cb_plain, cb_group = joblib.load(\"artifacts/cb_model.pkl\")\n",
    "    lgb_plain, lgb_group = joblib.load(\"artifacts/lgb_model.pkl\")\n",
    "    stacker = joblib.load(\"artifacts/stacker.pkl\")\n",
    "\n",
    "    # Feature engineering (reuse training logic)\n",
    "    train_processed, test_processed = fe.transform(train_df, test_df)\n",
    "\n",
    "    # Predictions\n",
    "    cb_preds = ensemble_predictions([cb_plain.get_test_preds(), cb_group.get_test_preds()])\n",
    "    lgb_preds = ensemble_predictions([lgb_plain.get_test_preds(), lgb_group.get_test_preds()])\n",
    "\n",
    "    # Stacking\n",
    "    # Get final stacked predictions\n",
    "    stacked_preds = stacker.get_test_preds()\n",
    "    \n",
    "    # Save\n",
    "    submission = pd.DataFrame({'ID': test_processed['ID'], 'Target': np.clip(stacked_preds, 0, stacked_preds.max())})\n",
    "    submission.to_csv(output_path, index=False)\n",
    "    print(f\"‚úÖ Inference complete. Predictions saved to {output_path}\")\n",
    "\n",
    "    print(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80d25464",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T20:02:18.754859Z",
     "iopub.status.busy": "2025-08-03T20:02:18.754566Z",
     "iopub.status.idle": "2025-08-03T20:02:18.760868Z",
     "shell.execute_reply": "2025-08-03T20:02:18.759696Z"
    },
    "papermill": {
     "duration": 0.012253,
     "end_time": "2025-08-03T20:02:18.762646",
     "exception": false,
     "start_time": "2025-08-03T20:02:18.750393",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.py\n",
    "import argparse\n",
    "from train import run_training, run_inference\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Run ML pipeline\")\n",
    "    parser.add_argument(\n",
    "        \"--mode\", choices=[\"train\", \"infer\"], required=True,\n",
    "        help=\"Mode: 'train' to train models, 'infer' to generate predictions\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output\", type=str, default=\"predictions.csv\",\n",
    "        help=\"Path to save predictions (default: predictions.csv)\"\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.mode == \"train\":\n",
    "        print(\"üöÄ Starting training...\")\n",
    "        run_training()\n",
    "    elif args.mode == \"infer\":\n",
    "        print(\"üîÆ Running inference...\")\n",
    "        run_inference(args.output)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b843073",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T20:02:18.771741Z",
     "iopub.status.busy": "2025-08-03T20:02:18.771409Z",
     "iopub.status.idle": "2025-08-03T20:03:00.552474Z",
     "shell.execute_reply": "2025-08-03T20:03:00.551258Z"
    },
    "papermill": {
     "duration": 41.787944,
     "end_time": "2025-08-03T20:03:00.554614",
     "exception": false,
     "start_time": "2025-08-03T20:02:18.766670",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting training...\r\n",
      "‚úÖ Data loaded successfully!\r\n",
      " - Train shape: (3579, 13)\r\n",
      " - Test shape: (1535, 12)\r\n",
      "artifacts/fe.pkl\r\n",
      "Training Folds: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:08<00:00,  1.20it/s]\r\n",
      "\r\n",
      "Mean CV RMSE: 6.7329\r\n",
      "Training Folds: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:08<00:00,  1.19it/s]\r\n",
      "\r\n",
      "Mean CV RMSE: 6.4441\r\n",
      "Training Folds: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:06<00:00,  1.61it/s]\r\n",
      "\r\n",
      "Mean CV RMSE: 6.7136 ¬± 1.1918\r\n",
      "Training Folds: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:05<00:00,  1.86it/s]\r\n",
      "\r\n",
      "Mean CV RMSE: 6.4691 ¬± 1.7824\r\n",
      "Stacking Folds: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 180.18it/s]\r\n",
      "\r\n",
      "Stacking CV RMSE: 6.6452 ¬± 1.1880\r\n",
      "‚úÖ Training complete. Models saved in 'artifacts/'.\r\n"
     ]
    }
   ],
   "source": [
    "!python main.py --mode train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "072546c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T20:03:00.568158Z",
     "iopub.status.busy": "2025-08-03T20:03:00.567798Z",
     "iopub.status.idle": "2025-08-03T20:03:05.321739Z",
     "shell.execute_reply": "2025-08-03T20:03:05.320584Z"
    },
    "papermill": {
     "duration": 4.762765,
     "end_time": "2025-08-03T20:03:05.323796",
     "exception": false,
     "start_time": "2025-08-03T20:03:00.561031",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÆ Running inference...\r\n",
      "‚úÖ Data loaded successfully!\r\n",
      " - Train shape: (3579, 13)\r\n",
      " - Test shape: (1535, 12)\r\n",
      "‚úÖ Inference complete. Predictions saved to predictions.csv\r\n",
      "                      ID     Target\r\n",
      "8     ID_AjcgJM_20100109   2.776282\r\n",
      "12    ID_1FILm2_20100113   9.799237\r\n",
      "14    ID_uRUSlU_20100115   3.835452\r\n",
      "17    ID_cJcBK7_20100118   3.669714\r\n",
      "19    ID_zEuY13_20100120   2.130194\r\n",
      "...                  ...        ...\r\n",
      "5096  ID_0y8uNw_20231215   4.008837\r\n",
      "5104  ID_Qj20ey_20231223   3.984340\r\n",
      "5110  ID_BgUL19_20231229  10.963587\r\n",
      "5112  ID_ow4iio_20231231   2.724987\r\n",
      "5113  ID_V5zPUV_20240101   3.361575\r\n",
      "\r\n",
      "[1535 rows x 2 columns]\r\n"
     ]
    }
   ],
   "source": [
    "!python main.py --mode infer --output predictions.csv"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7842386,
     "sourceId": 12432861,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 52.341942,
   "end_time": "2025-08-03T20:03:05.751027",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-03T20:02:13.409085",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
